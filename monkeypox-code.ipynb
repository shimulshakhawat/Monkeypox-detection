{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms as T # for simplifying the transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split\nfrom torchvision import models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Data Visualization\nimport plotly.express as px","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom tqdm import tqdm\nimport time\nimport copy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_classes(data_dir):\n    all_data = datasets.ImageFolder(data_dir)\n    return all_data.classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.datasets as datasets\nimport numpy as np\n\n# Set seed for reproducibility\nseed = 12\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\ndef get_data_loaders(data_dir, batch_size):\n    transform = T.Compose([\n        T.RandomHorizontalFlip(),\n        T.RandomVerticalFlip(),\n        T.RandomRotation(degrees=15),  # Added random rotation within Â±15 degrees\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    full_dataset = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/monkeypox-aug-munim/Monkeypox_Aug/\"), transform=transform)\n\n    # Calculate the sizes of the training and validation sets\n    dataset_size = len(full_dataset)\n    train_size = int(0.8 * dataset_size)\n    val_size = dataset_size - train_size\n\n    # Manually split the dataset into training and validation sets\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    test_transform = T.Compose([\n        T.ToTensor(),\n        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n\n    test_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/monkeypox-aug-munim/Monkeypox_Aug/\"), transform=test_transform)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return train_loader, val_loader, test_loader, len(train_dataset), len(val_dataset), len(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = \"/kaggle/input/monkeypox-aug-munim/Monkeypox_Aug/\"\nbatch_size = 128\ntrain_loader, val_loader, test_loader, train_size, val_size, test_size = get_data_loaders(data_dir, batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = get_classes(\"/kaggle/input/monkeypox-aug-munim/Monkeypox_Aug/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get Class Names\nclass_names = sorted(os.listdir(data_dir))\nn_classes = len(class_names)\n\n# Show\nprint(f\"Class Names : {class_names}\")\nprint(f\"Number of Classes  : {n_classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate class distribution\nclass_dis = [len(os.listdir(data_dir + name)) for name in class_names]\nclass_dis","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\nfig = px.pie(names=class_names, values=class_dis, title=\"Training Class Distribution\")\nfig.update_layout({'title':{'x':0.45}})\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_dataset_path = \"/kaggle/input/monkeypox-aug-munim/Monkeypox_Aug/\"\n# Calculate class distribution\nclass_dis = [len(os.listdir(pred_dataset_path + name)) for name in class_names]\n\n# Visualization\nfig = px.pie(names=class_names, values=class_dis, title=\"Prediction Class Distribution\")\nfig.update_layout({'title':{'x':0.45}})\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloaders = {\n    \"train\": train_loader,\n    \"val\": val_loader\n}\n\ndataset_sizes = {\n    \"train\": train_size,\n    \"val\": val_size\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_loader), len(val_loader), len(test_loader))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train dataset size:\", train_size)\nprint(\"Validation dataset size:\", val_size)\nprint(\"Test dataset size:\", test_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install timm # kaggle doesnt have it installed by default\nimport timm\nmodel = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n# Assuming 'model' is your existing model\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.head.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.head= nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, len(classes)),\n    nn.Softmax(dim=1)  # Softmax activation function\n)\n\nmodel = model.to(device)\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n# Add weight decay to the optimizer\nweight_decay = 1e-5  # You can adjust this value based on your needs\noptimizer = optim.AdamW(model.head.parameters(), lr=0.001, weight_decay=weight_decay)\n# Learning rate scheduler\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)\n\ncriterion = nn.CrossEntropyLoss()\ncriterion = criterion.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, f1_score, classification_report, cohen_kappa_score, roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport time\nimport copy\nimport torch\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n!pip install seaborn\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report, cohen_kappa_score, roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\n\nimport time\nimport copy\nimport torch\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, classes, device,\n                num_epochs=200, patience=500, save_weights_every=199):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    no_improvement_count = 0\n\n    val_true_labels = []\n    val_pred_labels = []\n    val_pred_probs = []\n    train_acc_history = []\n    val_acc_history = []\n    train_loss_history = []\n    val_loss_history = []\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\" * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0.0\n\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                if phase == 'val':\n                    val_true_labels += labels.tolist()\n                    val_pred_labels += preds.tolist()\n                    val_pred_probs += torch.softmax(outputs, dim=1).tolist()\n\n            if phase == 'train':\n                train_loss_history.append(running_loss / dataset_sizes[phase])\n            else:\n                val_loss_history.append(running_loss / dataset_sizes[phase])\n\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, running_loss / dataset_sizes[phase], epoch_acc))\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())  # keep the best validation accuracy model\n                no_improvement_count = 0\n            elif phase == 'val':\n                no_improvement_count += 1\n\n            if phase == 'train':\n                train_acc_history.append(epoch_acc.item())\n            else:\n                val_acc_history.append(epoch_acc.item())\n\n        print()\n\n        if no_improvement_count >= patience:\n            print(f\"No improvement in validation accuracy for {no_improvement_count} epochs. Early stopping...\")\n            break\n\n        # Save weights after every 'save_weights_every' epochs\n        if epoch % save_weights_every == 0 and epoch > 0:\n            save_path = f'model_weights_epoch_{epoch}.pth'\n            torch.save(model.state_dict(), save_path)\n            print(f'Model weights saved at epoch {epoch} to {save_path}')\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n\n    model.load_state_dict(best_model_wts)\n\n\n\n    # Calculate confusion matrix, F1 score, sensitivity, specificity, accuracy, and Cohen's kappa\n    confusion_mat = confusion_matrix(val_true_labels, val_pred_labels)\n    f1 = f1_score(val_true_labels, val_pred_labels, average='weighted')\n    sensitivity = confusion_mat.diagonal() / confusion_mat.sum(axis=1)\n    specificity = np.diag(confusion_mat) / np.sum(confusion_mat, axis=1)\n    accuracy = np.sum(np.diag(confusion_mat)) / np.sum(confusion_mat)\n    true_positives = np.diag(confusion_mat)\n    true_negatives = np.sum(confusion_mat) - (np.sum(true_positives) + np.sum(confusion_mat.sum(axis=0)) - np.sum(true_positives))\n    false_positives = confusion_mat.sum(axis=0) - true_positives\n    false_negatives = confusion_mat.sum(axis=1) - true_positives\n    kappa = cohen_kappa_score(val_true_labels, val_pred_labels)\n\n    # Print confusion matrix, F1 score, sensitivity, specificity, accuracy, and Cohen's kappa\n    print(\"Confusion Matrix:\")\n    print(confusion_mat)\n    print(\"F1 Score: {:.4f}\".format(f1))\n    print(\"Sensitivity (Recall):\", sensitivity)\n    print(\"Specificity:\", specificity)\n    print(\"Accuracy: {:.4f}\".format(accuracy))\n    print(\"True Positives:\", true_positives)\n    print(\"True Negatives:\", true_negatives)\n    print(\"False Positives:\", false_positives)\n    print(\"False Negatives:\", false_negatives)\n    print(\"Cohen's Kappa:\", kappa)\n\n\n    # Print classification report\n    target_names = [str(i) for i in range(len(classes))]\n    print(classification_report(val_true_labels, val_pred_labels, target_names=target_names))\n    \n    \n    \n    # Plot accuracy curves\n    plt.figure(figsize=(10, 5))\n    sns.lineplot(x=range(1, len(train_acc_history) + 1), y=train_acc_history, label='Train', linestyle='-', color='#DC8686')\n    sns.lineplot(x=range(1, len(val_acc_history) + 1), y=val_acc_history, label='Validation', linestyle='-', color='#59CE8F')\n\n    # Create a band around the train line\n    plt.fill_between(range(1, len(train_acc_history) + 1), np.array(train_acc_history) - 0.02, np.array(train_acc_history) + 0.02, color='#DC8686', alpha=0.2)\n\n    # Create a band around the validation line\n    plt.fill_between(range(1, len(val_acc_history) + 1), np.array(val_acc_history) - 0.02, np.array(val_acc_history) + 0.02, color='#59CE8F', alpha=0.2)\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    plt.show()\n\n    # Plot loss curves\n    plt.figure(figsize=(10, 5))\n    sns.lineplot(x=range(1, len(train_loss_history) + 1), y=train_loss_history, label='Train', linestyle='-', color='#DC8686')\n    sns.lineplot(x=range(1, len(val_loss_history) + 1), y=val_loss_history, label='Validation', linestyle='-', color='#59CE8F')\n\n    # Create a band around the train line\n    plt.fill_between(range(1, len(train_loss_history) + 1), np.array(train_loss_history) - 0.02, np.array(train_loss_history) + 0.02, color='#DC8686', alpha=0.2)\n\n    # Create a band around the validation line\n    plt.fill_between(range(1, len(val_loss_history) + 1), np.array(val_loss_history) - 0.02, np.array(val_loss_history) + 0.02, color='#59CE8F', alpha=0.2)\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.show()\n\n\n\n\n    confusion_mat = confusion_matrix(val_true_labels, val_pred_labels, labels=np.arange(len(classes)))\n    # Plot customized confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(confusion_mat, annot=True, fmt=\".0f\", cmap=\"GnBu\", linewidths=.5, square=True, cbar=False,\n                xticklabels=classes, yticklabels=classes)\n    \n    plt.title('Confusion Matrix', fontsize=16)\n    plt.xlabel('Predicted label', fontsize=12)\n    plt.ylabel('True label', fontsize=12)\n    \n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0, ha='right')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, dataloaders, dataset_sizes, classes, device)\nexample = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(model.cpu(), example)\ntraced_script_module.save(\"/kaggle/working/monkeypox_deit.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}